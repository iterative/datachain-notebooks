{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs and RAG with DataChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LLM applications nowadays, the emerging standard pattern for most use-cases is to employ a pre-trained model with an API from a 3rd party provider and to augment it with a RAG context. Naive application of \"latest and greatest\" models with no prompt engineering, testing and evaluation of RAG context can lead to needlessly expensive operational costs at best and dissapointingly poor performance at worst.\n",
    "\n",
    "Therefore, just like in machine learning training, we need to version all that data as we finetune our applications to be able to correctly evaluate the effect of any changes we apply to our models. We can experiment with the LLM choice, prompt engineering, the way we process data for our RAG context (pre-processing, embedding, ...) and so on.\n",
    "\n",
    "In this example, we will see how we can use DataChain to create such a controlled development environment and how it can help us when we evaluate any fine-tuning of our LLM applications.\n",
    "\n",
    "We will see how to use DataChain to version our RAG context datasets to preserve reproducibility of our fine-tuning experiments as the RAG context changes. We will also see how to use DataChain in the evaluation of fine-tuning by comparing two different text embedding models and saving (and versioning) the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a large collection of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have a collection of relevant documents which we want to use as context in LLM queries in our chatbot application. We will be using DataChain to create, store and version vector embeddings of our documents.\n",
    "\n",
    "In this example we will be using papers from the [Neural Information Processing Systems](https://papers.neurips.cc/paper/) conference. \n",
    "\n",
    "We will proceed in the following steps:\n",
    "1. [Data ingestion with DataChain](#data-ingestion)\n",
    "1. [Data processing with the Unstructured Python library](#processing-the-documents-individually)\n",
    "1. [Scaling the data processing with DataChain](#processing-the-documents-at-scale-using-datachain-udfs)\n",
    "1. [Using Datachain to evaluate different embedding models](#evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import func\n",
    "from sqlalchemy import cast\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from datachain.lib.dc import DataChain, C\n",
    "from datachain.sql.types import Float\n",
    "from datachain.lib.data_model import DataModel\n",
    "from datachain.lib.file import File\n",
    "from datachain.sql.functions.array import cosine_distance\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "from unstructured.embed.huggingface import HuggingFaceEmbeddingConfig, HuggingFaceEmbeddingEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first ingest the dataset. The data are saved on a cloud storage, so we use the `.from_storage` DataChain method. We will also use the `.filter` method to restrict ourselves only to `.pdf` files (the storage contains many other data which we do not need).\n",
    "\n",
    "Notice that:\n",
    "\n",
    "1. Since DataChain employs lazy evaluation, no data are actually loaded just yet (until we invoke an action such as showing or saving our DataChain)\n",
    "1. The previous point also means that when we filter out all non-pdf files, DataChain doesn't actually waste time loading their content only to throw them away later. This makes DataChain a lot more scalable than tools with eager evaluation.\n",
    "1. The `.from_storage` method of DataChain operates on the level of the entire bucket. This means that even if the files are stored using a complicated directory structure and potentially uploaded irregularly into this structure, we can retrieve or update our DataChain of articles with just a simple one-line command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_papers = (\n",
    "    DataChain.from_storage(\"gs://datachain-demo/neurips\")\n",
    "    .filter(C.name.glob(\"*.pdf\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Listing gs://datachain-demo: 738 objects [00:00, 871.19 objects/s]\n",
      "Processed: 738 rows [00:00, 10006.87 rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>parent</th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>version</th>\n",
       "      <th>etag</th>\n",
       "      <th>is_latest</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>location</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs://datachain-demo</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf</td>\n",
       "      <td>2291566</td>\n",
       "      <td>1721047139405563</td>\n",
       "      <td>CPudi5uIqYcDEAE=</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-07-15 12:38:59.443000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs://datachain-demo</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>03afdbd66e7929b125f8597834fa83a4-Paper.pdf</td>\n",
       "      <td>1322648</td>\n",
       "      <td>1721047138865046</td>\n",
       "      <td>CJaf6pqIqYcDEAE=</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-07-15 12:38:58.917000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gs://datachain-demo</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>072b030ba126b2f4b2374f342be9ed44-Paper.pdf</td>\n",
       "      <td>1220711</td>\n",
       "      <td>1721046993295769</td>\n",
       "      <td>CJmztdWHqYcDEAE=</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-07-15 12:36:33.340000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file               file  \\\n",
       "                source             parent   \n",
       "0  gs://datachain-demo  neurips/1987/file   \n",
       "1  gs://datachain-demo  neurips/1987/file   \n",
       "2  gs://datachain-demo  neurips/1987/file   \n",
       "\n",
       "                                         file     file              file  \\\n",
       "                                         name     size           version   \n",
       "0  02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf  2291566  1721047139405563   \n",
       "1  03afdbd66e7929b125f8597834fa83a4-Paper.pdf  1322648  1721047138865046   \n",
       "2  072b030ba126b2f4b2374f342be9ed44-Paper.pdf  1220711  1721046993295769   \n",
       "\n",
       "               file      file                             file     file  file  \n",
       "               etag is_latest                    last_modified location vtype  \n",
       "0  CPudi5uIqYcDEAE=         1 2024-07-15 12:38:59.443000+00:00     None        \n",
       "1  CJaf6pqIqYcDEAE=         1 2024-07-15 12:38:58.917000+00:00     None        \n",
       "2  CJmztdWHqYcDEAE=         1 2024-07-15 12:36:33.340000+00:00     None        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Limited by 3 rows]\n"
     ]
    }
   ],
   "source": [
    "dc_papers.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataChain created a record for each `pdf` file in the `neurips` directory, generating a `file` signal for each file. The file signal contains subsignals with metadata about each file, like `file.name` and `file.size`. Aggregate signals like `file` that contain multiple subsignals are called features.\n",
    "\n",
    "You can use the `file` feature to not only get metadata about each file, but also to open and read the file as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the documents individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to ingest the content of the pdf files as text, divide it into chunks and vectorize those for our RAG application. We are interested in comparing two different models for embeddings. Normally, we would also do some pre-processing and cleaning of the text before vectorization, but we will skip it here for brevity.\n",
    "\n",
    "We will first do all this with an example of a single pdf using the `unstructured` Python library and then we will see how we can scale this up to the entire bucket with the help of DataChain.\n",
    "\n",
    "First, we ingest and partition the pdf file and chunk it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_by_title(partition_pdf(filename=\"sample.pdf\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we vectorize each chunk using HuggingFace embedding encoders. Ideally, we want the smallest model possible while maintaining accuracy to increase speed and reduce costs of embeddings. We will see how embeddings from a candidate model `MODEL_NEW` differ from embeddings produced by the existing model `MODEL_OLD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibor/Repos/datachain/.venv/lib64/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NEW = \"sentence-transformers/paraphrase-MiniLM-L6-v2\" \n",
    "MODEL_OLD = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embedding_encoder_new = HuggingFaceEmbeddingEncoder(\n",
    "     config=HuggingFaceEmbeddingConfig(model_name=MODEL_NEW, encode_kwargs={\"normalize_embeddings\":True})\n",
    ")\n",
    "\n",
    "chunks_embedded_new = embedding_encoder_new.embed_documents(chunks)\n",
    "\n",
    "embedding_encoder_old = HuggingFaceEmbeddingEncoder(\n",
    "     config=HuggingFaceEmbeddingConfig(model_name=MODEL_OLD, encode_kwargs={\"normalize_embeddings\":True})\n",
    ")\n",
    "\n",
    "# we need deepcopy here because unstructured creates lists of references to elements\n",
    "chunks_embedded_old = embedding_encoder_old.embed_documents(deepcopy(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our chunks vectorized and ready for comparison (e.g. with cosine similarity). However, we are missing a few ingredients:\n",
    "\n",
    "1. ***Scaling*** - we only processed a single pdf file and we had to manually specify its path. We need to find a way to process all our documents at scale instead and to save the results.\n",
    "2. ***Saving and Versioning*** - even if we only had a single or a few PDF files we would like to use in our RAG, it is a good practice to version the outputs so that we can keep track of and fine-tune our RAG application. If we simply save the current results to a bucket and overwrite it each time the source is updated, we lose this. We could version the results manually, e.g. by adding a timestamp to the blob name, but that is not very reliable and will lead to unnecessary copies of files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the documents at scale, using DataChain UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use DataChain to solve the scaling and versioning issues we outline above. We will create a DataChain user-defined function (UDF) to process all our PDF files the way we did above with a single file (without us having to manually provide file paths) and save the outputs in a Datachain.\n",
    "\n",
    "The DataChain UDF functionality will allow us to generate additonal columns in our DataChain, iterating over each of the files listed in it.\n",
    "\n",
    "We first need to define a DataModel class, which will define the types of our outputs. Inputs and outputs need to be specified like this when we use custom functinos in Datachain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output as a Feature class\n",
    "class Chunk(DataModel):\n",
    "    key: str\n",
    "    text: str\n",
    "    embeddings_new: list[float]\n",
    "    embeddings_old: list[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we define `Chunk` by specifying the names and types of new columns on the output.\n",
    "\n",
    "We then define our processing function `pdf_chuks`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use signatures to define input/output types (these can be Feature or regular Python types)\n",
    "def process_pdf(file: File) -> Iterator[Chunk]:\n",
    "    # Ingest the file\n",
    "    with file.open() as f:\n",
    "        chunks = partition_pdf(file=f, chunking_strategy=\"by_title\")\n",
    "\n",
    "    chunks_embedded_new = embedding_encoder_new.embed_documents(chunks)\n",
    "    chunks_embedded_old = embedding_encoder_old.embed_documents(deepcopy(chunks))\n",
    "\n",
    "    # Add new rows to DataChain\n",
    "    for chunk, chunk_orig in zip(chunks_embedded_new, chunks_embedded_old):\n",
    "        yield Chunk(\n",
    "            key=file.name.removesuffix(\"-Paper.pdf\"),\n",
    "            text=chunk.text,\n",
    "            embeddings_new=chunk.embeddings,\n",
    "            embeddings_old=chunk_orig.embeddings,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the syntax is the same as with any other Python function, except that we specify the input and output types using type hints\n",
    "\n",
    "```\n",
    "def process_pdf(file: File) -> Iterator[Chunk]:\n",
    "```\n",
    "Here, `file` specifies that we pass all `file` columns of the original dataset on the input and `Iterator[Chunk]` specifies that we get a bunch of `Chunk` rows on the output (from a single row of the original datachain representing a single paper we will get a new dataset with multiple rows per paper, each representing a single chunk).\n",
    "\n",
    "We then specify what each row should contain by specifying the attributes of our `Chunk` class and then we use `yield` to create the new rows for each input row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 738 rows [00:00, 9812.89 rows/s]\n",
      "Processed: 0 rows [00:00, ? rows/s]\n",
      "Processed: 2 rows [00:05,  2.68s/ rows]\n",
      "Processed: 3 rows [00:09,  3.28s/ rows]\n",
      "Processed: 4 rows [00:13,  3.62s/ rows]\n",
      "Processed: 5 rows [00:19,  4.44s/ rows]\n",
      "Processed: 6 rows [00:24,  4.54s/ rows]\n",
      "Processed: 7 rows [00:28,  4.33s/ rows]\n",
      "Processed: 8 rows [00:33,  4.55s/ rows]\n",
      "Processed: 9 rows [00:38,  4.75s/ rows]\n",
      "Processed: 10 rows [00:43,  4.80s/ rows]\n",
      "Processed: 11 rows [00:50,  5.49s/ rows]\n",
      "Processed: 12 rows [00:55,  5.35s/ rows]\n",
      "Processed: 13 rows [01:00,  5.37s/ rows]\n",
      "Processed: 14 rows [01:05,  5.25s/ rows]\n",
      "Processed: 15 rows [01:09,  4.70s/ rows]\n",
      "Processed: 16 rows [01:14,  4.93s/ rows]\n",
      "Processed: 17 rows [01:18,  4.57s/ rows]\n",
      "Processed: 18 rows [01:22,  4.32s/ rows]\n",
      "Processed: 19 rows [01:27,  4.65s/ rows]\n",
      "Processed: 20 rows [01:32,  4.82s/ rows]\n",
      "Download: 51.2MB [01:37, 552kB/s]\n",
      "Processed: 20 rows [01:37,  4.88s/ rows]\n",
      "Generated: 1346 rows [01:32, 14.59 rows/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datachain.lib.dc.DataChain at 0x7fdc4684b9e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_chunks_embeddings = (\n",
    "    dc_papers\n",
    "    .limit(20) # we limit ourselves to 20 papers here, to speed up the demo\n",
    "    .gen(document=process_pdf)\n",
    ")\n",
    "\n",
    "dc_chunks_embeddings.save(\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we apply our new `process_pdf` function to the DataChain `dc_papers`. We do that by using the `gen` method of DataChain with `process_pdf`as its parameter. \n",
    "\n",
    "`DataChain.gen` is used when we have a function that creates multiple rows per single row of the original datachain (like in our examples, where each paper is split into multiple chunks)\n",
    "\n",
    "We also presisted the result by the `.save` method. This will permanently save and version the datachain as a dataset with the name `embeddings`. Whenever we call `.save(\"embeddings\")` again, a new version of this dataset will be saved automatically, so we can recall previous versions and track changes of the dataset over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use DataChain to calculate similarity between the two alternative embeddings using a fixed test query as reference and for further evaluation we will save dataset containing the chunks where the two embeddings differ the most.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUERY = \"What are the most promising approaches for combining neural networks with symbolic reasoning, according to recent NeurIPS papers?\"\n",
    "\n",
    "embedded_query_new = embedding_encoder_new.embed_query(query = TEST_QUERY)\n",
    "embedded_query_old = embedding_encoder_old.embed_query(query = TEST_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the built-in DataChain function `cosine_distance` we will calculate the cosine similarities between each chunk and the test query `TEST_QUERY` and then compare the results between the two embeddings.\n",
    "\n",
    "To specify that we want to compare columns we use the `C` class from `datachain.lib.dc`. We use the `mutate` method of DataChain, which is a way to add new columns to an existing dataset.\n",
    "\n",
    "Since we saved our dataset `embeddings`, we can now load its content to datachain by the `from_dataset` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_differences = (\n",
    "    DataChain\n",
    "    .from_dataset(\"embeddings\")\n",
    "    .mutate(\n",
    "        query_sim_new = 1 - cosine_distance(C.document.embeddings_new, embedded_query_new),\n",
    "        query_sim_old = 1 - cosine_distance(C.document.embeddings_old, embedded_query_old),\n",
    "        )\n",
    "    .mutate(abs_difference = cast(func.abs(C.query_sim_old - C.query_sim_new), Float))\n",
    "    .filter(C.abs_difference > 0.1)\n",
    "    .order_by(\"abs_difference\", descending=True)\n",
    "    .save(\"embeddings-differences\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>document</th>\n",
       "      <th>document</th>\n",
       "      <th>document</th>\n",
       "      <th>query_sim_new</th>\n",
       "      <th>query_sim_old</th>\n",
       "      <th>abs_difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings_new</th>\n",
       "      <th>embeddings_old</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32bb90e8976aab5298d5da10fe66f21d</td>\n",
       "      <td>value a unit takes on after probing becomes de...</td>\n",
       "      <td>[-0.029339559376239777, -0.008152788504958153,...</td>\n",
       "      <td>[-0.04788152500987053, -0.016090188175439835, ...</td>\n",
       "      <td>0.341957</td>\n",
       "      <td>-0.018575</td>\n",
       "      <td>0.360531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03afdbd66e7929b125f8597834fa83a4</td>\n",
       "      <td>To evaluate E(ln - h2- K I), we estimate the v...</td>\n",
       "      <td>[-0.010849174112081528, -0.025998728349804878,...</td>\n",
       "      <td>[0.007942826487123966, 0.03667686879634857, 0....</td>\n",
       "      <td>0.223943</td>\n",
       "      <td>-0.080508</td>\n",
       "      <td>0.304451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f0e3dad99908345f7439f8ffabdffc4</td>\n",
       "      <td>[3] Brody D.A., IEEE Trans. vBME-32, n2, pl06-...</td>\n",
       "      <td>[-0.12260544300079346, 0.07194254547357559, -0...</td>\n",
       "      <td>[-0.11976780742406845, -0.06083959713578224, -...</td>\n",
       "      <td>0.043594</td>\n",
       "      <td>0.339409</td>\n",
       "      <td>0.295815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           document  \\\n",
       "                                key   \n",
       "0  32bb90e8976aab5298d5da10fe66f21d   \n",
       "1  03afdbd66e7929b125f8597834fa83a4   \n",
       "2  1f0e3dad99908345f7439f8ffabdffc4   \n",
       "\n",
       "                                            document  \\\n",
       "                                                text   \n",
       "0  value a unit takes on after probing becomes de...   \n",
       "1  To evaluate E(ln - h2- K I), we estimate the v...   \n",
       "2  [3] Brody D.A., IEEE Trans. vBME-32, n2, pl06-...   \n",
       "\n",
       "                                            document  \\\n",
       "                                      embeddings_new   \n",
       "0  [-0.029339559376239777, -0.008152788504958153,...   \n",
       "1  [-0.010849174112081528, -0.025998728349804878,...   \n",
       "2  [-0.12260544300079346, 0.07194254547357559, -0...   \n",
       "\n",
       "                                            document query_sim_new  \\\n",
       "                                      embeddings_old                 \n",
       "0  [-0.04788152500987053, -0.016090188175439835, ...      0.341957   \n",
       "1  [0.007942826487123966, 0.03667686879634857, 0....      0.223943   \n",
       "2  [-0.11976780742406845, -0.06083959713578224, -...      0.043594   \n",
       "\n",
       "  query_sim_old abs_difference  \n",
       "                                \n",
       "0     -0.018575       0.360531  \n",
       "1     -0.080508       0.304451  \n",
       "2      0.339409       0.295815  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Limited by 3 rows]\n"
     ]
    }
   ],
   "source": [
    "embeddings_differences.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore where our old and new embeddings differ the most in terms of their distance to the test query. We are mostly curious about how the RAG context provided changes when we change the embedding, so we will have a look at how the sets of closest 10 chunks differ between the embeddings.\n",
    "\n",
    "We use the `.collect` method of DataChain to retrieve a set of the 10 most relevant chunks (since we will want to have a look at them in detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RELEVANT = 10\n",
    "\n",
    "top_old = set(embeddings_differences\n",
    "               .order_by(\"query_sim_old\", descending=True)\n",
    "               .limit(N_RELEVANT)\n",
    "               .select(\"document.text\")\n",
    "               .collect()\n",
    "               )\n",
    "top_new = set(embeddings_differences\n",
    "               .order_by(\"query_sim_new\", descending=True)\n",
    "               .limit(N_RELEVANT)\n",
    "               .select(\"document.text\")\n",
    "               .collect()\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple metric we call retrieval similarity with values between 0 and 1. If the retrieval sets were the same for both embeddings, its value would be 1. If they were completely different, the value would be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "retrieval_similarity = 1- len(top_old ^ top_new) / (2 * N_RELEVANT)\n",
    "print(retrieval_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a substantial difference between the two embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to get a bit more insight into how the two retrieval sets differ, we can have a look at what context appears in one set but not the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('In the present paper we survey and utilize results from the qualitative theory of large scale interconnected dynamical systems in order to develop a qualitative theory for the Hopfield model of neural networks. In our approach we view such networks as an inter connection of many single neurons. Our results are phrased in terms of the qualitative properties of the individual neurons and in terms of the properties of the interconnecting structure of the neural networks. Aspects of neural networks',),\n",
       " ('This research was also sponsored by the same agency under contract N00039-87-\\n\\nC-0251 and monitored by the Space and Naval Warfare Systems Command.\\n\\n231\\n\\n232\\n\\nReferences\\n\\n[1] J. J. Hopfield, \"Neural networks and physical systems with emergent collective computational abilities,\" Proceedings of the National Academy of Sciences U.SA., vol. 79, pp. 2554-2558, April 1982.\\n\\n[2] J. Hopfield and D. Tank, \\'\\'\\'Neural\\' computation of decisions in optimization',),\n",
       " ('boolean computation, such as described by McCulloch and Pitts16, since it is bit serial. Neural net models using integers and floating point arithmetic 17,18 will also work but will be somewhat slower since the time for computation is proportional to the number of bits of the operands.',),\n",
       " ('in contrast to real valued inputs that come from, say, a chaotic time series, the input points in symbolic processing problems are widely separated and the bumps do not add together to form smooth surfaces. Furthermore, each input bit string is a corner of an 2N vertex hypercube, and there is no sense in which one corner of a hypercube is surrounded by the other corners. Thus the commonly used input representation for symbolic processing problems requires that the neural net extrapolate the',),\n",
       " ('into general decision regions. We are therefore able to conclude that the network architecture consisting of just two hidden layers is sufficient for learning any symbol processing training set. For Boolean symbol mappings one need not use the second hidden layer to remove the saddles on the bump (c.f. Fig. 6). The saddles are lower than the central maximum so one may choose a threshold on the output layer to cut the bump at a point over the saddles to yield the correct decision region. Whether',),\n",
       " ('the connections between neurons. This is done using a software scheme first pre sented in 11,20. The original method was intended for realizing directed graphs in SIMD architectures. Since a neural network is a graph with the neurons being vertices and the connections being arcs, the method maps perfectly to this system. Henceforth the terms neuron and vertex and the terms arc and connection will be used interchangeably.',)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(top_old) - set(top_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('474\\n\\nOPTIMIZA nON WITH ARTIFICIAL NEURAL NETWORK SYSTEMS: A MAPPING PRINCIPLE AND A COMPARISON TO GRADIENT BASED METHODS t\\n\\nHarrison MonFook Leong Research Institute for Advanced Computer Science NASA Ames Research Center 230-5 Moffett Field, CA, 94035\\n\\nABSTRACT',),\n",
       " ('Neural nets, in contrast to popular misconception, are capable of quite accurate number crunching, with an accuracy for the prediction problem we considered that exceeds conventional methods by orders of magnitude. Neural nets work by constructing surfaces in a high dimensional space, and their oper ation when performing signal processing tasks on real valued inputs, is closely related to standard methods of functional ,,-pproximation. One does not need more than two hidden layers for processing',),\n",
       " ('Neural networks have attracted much interest recently, and using parallel architectures to simulate neural networks is a natural and necessary applica tion. The SIMD model of parallel computation is chosen, because systems of this type can be built with large numbers of processing elements. However, such systems are not naturally suited to generalized communication. A method is proposed that allows an implementation of neural network connections on massively parallel SIMD architectures. The key',),\n",
       " ('Recent neural or connectionist models are based on a common structure, that of highly interconnected networks of linear (or polynomial) threshold (or with sig moid input-output function) units with adjustable interconnection weights. We shall therefore review the complexity theory of such circuits. In doing so, it will be some times helpful to contrast it with the similar theory based on Boolean (AND, OR, NOT) gates. The presentation will be rather informal and technical complements can easily',),\n",
       " ('To be more specific, we assume that a given neural network has been designed with a set of interconnections whose strengths can be varied from zero to some specified values. We express this by writing in place of (1),\\n\\nN\\n\\nXi = -biXi + L:8ij Aij Gj(Xj) + Ui(t),\\n\\nfor i = 1, ... ,N,',),\n",
       " ('neural systems.',)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(top_new) - set(top_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a casual look, it seems that our old embedding model does a better job at picking the right chunks - all of the chunks which are unique to the old model seem to contain relevant text, wereas two of the 6 unique snippets from the new model are just a title and a two word phrase, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have now solved our scalability issues. When using `DataChain` locally, our computation will still be restricted to a simgle machine but for larger datasets you can use the SaaS version of DataChain available through our DVC Studio which comes with automatic computation cluster management, a graphical user interface and additional ML and data versioning features.\n",
    "\n",
    "We have also solved our versioning needs and we can track the differences between embeddings over time and use that to choose the best embedding for our use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to go from here?\n",
    "\n",
    "To turn this example into a real-world scenario we might first want to use datachain to create a dataset of most common queries (instead of just the one test query above) and then use an averaged out retrieval similarity metric for the two embeddings across all these common queries. This would give us a good way to judge whether we can replace the current embedding model (which is presumably more expensive) with a faster/cheaper one without affecting the accuracy of our RAG much.\n",
    "\n",
    "Also, from a detailed look at our retrieval results, it is clear that we can improve the results by improving the PDF processing itself - cleaning it and trying out various chunking strategies can lead to more relevant context before we even consider changing the embedding model itself. There are many potential combinations of data processing/cleaning strategies, embedding models and their parameters (and also the choice of th corpus of text we use as our RAG  context). \n",
    "\n",
    "DataChain can be of tremendous help with this experimentation, since it helps us to:\n",
    "\n",
    "1. Do all of that data processing at scale\n",
    "1. Provide versioning and reproducibility necessary to systematically arrive at the best possible RAG configuration for our use-case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
